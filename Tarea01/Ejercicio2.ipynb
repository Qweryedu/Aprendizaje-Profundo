{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tarea 1 - Redes densas\n",
    "## Eduardo García Alarcón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retropropagación en red densa\n",
    "\n",
    "Programa el algoritmo de retropropagación usando NumPy para una tarea de clasificación binaria presuponiendo una red densa con dos capas ocultas. Las neuronas de las capas ocultas cuentan con una función de activasión ReLU, definida por $$ ReLU(x) = máx(0,z)$$\n",
    "\n",
    "Por su parte, la capa de salida está compuesta por una sola neurona logística.\n",
    "\n",
    "Para el entrenamiento minimiza el promedio de la función de pérdida de entropía cruzada binaria: \n",
    "$$\n",
    "ECB(\\bold{y}, \\bold{\\hat y}) = -\\frac{1}{n} \\sum_{i=1}^n \\big[ y^{(i)} log \\big(\\hat y^{(i)}\\big) + \\big(1-y^{(i)}\\big) log\\big(1-\\hat y^{(i)}\\big)\\big]\n",
    "$$\n",
    "\n",
    "Entrena la red mediante descenso por gradiente y el algoritmo de retropropagación de errores.\n",
    "\n",
    "Describe las fórmulas y reglas de actualización de los pesos y sesgos de cada capa y entrena y evalúa la red en algún problema de clasifiación no lineal. Compara el comportamiento del entrenamiento de esta red con una en la que las neuronas de las capas ocultas tienen una función de activasión logística y en la que la función de pérdida no se promedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilbiotecas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la función de ReLU y su derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "  return np.max([0.0, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_relu(x):\n",
    "  if x < 0.0:\n",
    "    return 0.0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De Igual manera la sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "  return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_sigmoide(x):\n",
    "  s = sigmoide(x)\n",
    "  return s * (1.0 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la función de Entropía Cruzada Binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia_cruzada_binaria(y, p):\n",
    "  # Para evitar divisiones entre 0\n",
    "  p[p == 0] = np.nextafter(0., 1.)\n",
    "  p[p == 1] = np.nextafter(1., 0.)\n",
    "  \n",
    "  return -( np.log( p[y==1] ).sum() + np.log(1 - p[y==0]).sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Y definimos al exactitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactitud(y, y_hat):\n",
    "  return (y == y_hat).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propagación"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
